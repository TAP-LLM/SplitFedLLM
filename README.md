# FedGLM: Safely Learning with Private Data
https://github.com/TAP-LLM/SplitFedLLM/assets/131137567/c647c889-4c9c-43e7-b82b-6d3fbb0638b1
## Reviewer TIPS
This is a project containing FL-GLM and FL-LLaMA. If you are a reviewer for <A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability> . Please click the FL-LLaMA directory. Thank you for your support! 


## Project Introduction
This open source project is based on the open source LLAMA and GLM models, and has implemented a distributed federated learning framework for model fine-tuning and inference on a single/multiple machine deployment.
While ensuring client data privacy and security, it aggregates model parameters to achieve model parameter sharing. This allows users with limited computing power to use the resources of the project deployment platform for model fine-tuning, thereby achieving vertical domain customization of the model.

2024/7/9: Due to project funding, the ChatGLM federated learning code is currently undergoing open-source approval. We will continue to provide updates.

2024/7/16: FedGLM releasedï¼

## Supported Models
| Model            | Type | Download                                                                                                                                |
|------------------|------|-----------------------------------------------------------------------------------------------------------------------------------------|
| ChatGLM-6B | Chat |https://github.com/THUDM/ChatGLM-6B|
| Llama-2-7b-hf    | Base | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
| Llama-2-7b-chat-hf | Chat | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |



## Notices
FedGLM is currently **going through the open-source procedures**. Once the process is completed, it will be uploaded to GitHub as soon as possible. In the meantime, if you need to use FedGLM, please send an email using your academic email address (e.g., @edu.cn, @edu.com) to [sy2342110@buaa.edu.cn], stating your name, the institution you are affiliated with, and the electronic PDF version of **the Commitment Letter**. Upon review, the code will be sent to you via email. Thank you for your understanding and support.

Please note that the FedGLM folder contains training solutions based on the flower framework and ChatGLM-6B, while the FedGLM-LaMA folder contains training solutions based on the flask communication framework and LLaMA-7B, for details, please refer to the readme file in each folder. We recommend using FedGLM because the communication capability of the flower framework is more stable. In addition, FedGLM supports training with both Chinese and English, but FedGLM-LaMA is more suitable for training English.

## Commitment Letter Template in Chinese

æºä»£ç ä½¿ç”¨æ‰¿è¯ºä¹¦

æˆ‘æ˜¯ [æ‚¨çš„å…¨å]ï¼Œæ¥è‡ª [æ‚¨çš„å­¦æ ¡/æœºæ„]ã€‚æˆ‘ç°æ­£ä»äº‹ [ç ”ç©¶é¢†åŸŸ/é¡¹ç›®åç§°] æ–¹é¢çš„ç ”ç©¶å·¥ä½œã€‚æœ¬äººå¯¹ä½ ä»¬å¼€å‘çš„é¡¹ç›®å¾ˆæ„Ÿå…´è¶£ï¼Œå¸Œæœ›è·å¾—è¯¥é¡¹ç›®çš„æºä»£ç ä»¥æ”¯æŒæˆ‘çš„ç ”ç©¶æ´»åŠ¨ã€‚ åœ¨æ­¤ï¼Œæˆ‘éƒ‘é‡æ‰¿è¯ºï¼š 
1. æˆ‘å°†ä»…åœ¨ä¸ªäººå­¦ä¹ ã€ç ”ç©¶æˆ–éå•†ä¸šç”¨é€”ä¸‹ä½¿ç”¨æ‰€è·å–çš„æºä»£ç ã€‚
2. æˆ‘ä¸ä¼šå°†æºä»£ç ç›´æ¥æˆ–é—´æ¥åœ°ç”¨äºä»»ä½•å•†ä¸šäº§å“æˆ–æœåŠ¡ä¸­ã€‚
3. æˆ‘ä¸ä¼šå°†æºä»£ç æ³„éœ²ç»™ä»»ä½•ç¬¬ä¸‰æ–¹ã€‚
4. è‹¥è¿åä¸Šè¿°ä»»ä¸€æ¡æ¬¾ï¼Œæˆ‘æ„¿æ„æ‰¿æ‹…ç”±æ­¤äº§ç”Ÿçš„ä¸€åˆ‡åæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ³•å¾‹è¿½è´£ã€‚

 æ‰¿è¯ºäººç­¾å: ______________________ æ—¥æœŸ: ________ å¹´ ___ æœˆ ___ æ—¥
 
 è”ç³»ä¿¡æ¯: - é‚®ç®±: [æ‚¨çš„é‚®ç®±] - ç”µè¯: [æ‚¨çš„ç”µè¯å·ç ]
 
 ## Commitment Letter Template in English
 
 Source Code Usage Commitment Letter
 
I, [Your Full Name], from [Your University/Institution], am currently engaged in research/work in the field of [Research Area/Project Name]. I am interested in your project and would like to obtain the source code to support my research activities.

Hereby, I solemnly commit to the following:

I will use the obtained source code solely for personal learning, research, or non-commercial purposes.
I will not use the source code directly or indirectly for any commercial products or services.
I will not disclose the source code to any third party.
If I violate any of the above terms, I am willing to bear all consequences, including but not limited to legal liabilities.
Signature: ______________________
Date: ________ Year ___ Month ___ Day

Contact Information:

Email: [Your Email]
Phone: [Your Phone Number]



## Reference
If you find this repository useful or our work is related to your research, please kindly cite it:
```
@misc{zheng2024safelylearningprivatedata,
      title={Safely Learning with Private Data: A Federated Learning Framework for Large Language Model}, 
      author={JiaYing Zheng and HaiNan Zhang and LingXiang Wang and WangJie Qiu and HongWei Zheng and ZhiMing Zheng},
      year={2024},
      eprint={2406.14898},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.14898}, 
}
```




