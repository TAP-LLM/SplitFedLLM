# SplitFedLLM
![transformers](https://img.shields.io/badge/transformers->=4.38.0-greene)
![License](https://img.shields.io/badge/license-MIT-yellow)  
![Python](https://img.shields.io/badge/Python->=3.10.4-blue)  
 Read this in [English](README_en.md)

## é¡¹ç›®ä»‹ç»
æœ¬å¼€æºé¡¹ç›®åŸºäºå¼€æºçš„LLAMAå’ŒGLMæ¨¡å‹ï¼Œå®ç°äº†å•æœº/å¤šæœºéƒ¨ç½²çš„åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ æ¡†æ¶å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥åŠæ¨ç†,  
åœ¨ä¿è¯å®¢æˆ·ç«¯æ•°æ®éšç§å®‰å…¨çš„åŒæ—¶ï¼Œå®ç°æ¨¡å‹å‚æ•°çš„èšåˆï¼Œä»è€Œå®ç°æ¨¡å‹å‚æ•°çš„å…±äº«ã€‚ä½¿å¾—ç”¨æˆ·å¯ä»¥åœ¨è‡ªèº«ç®—åŠ›  
æœ‰é™çš„æƒ…å†µä¸‹åˆ©ç”¨é¡¹ç›®éƒ¨ç½²å¹³å°çš„èµ„æºç«¯è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œä»è€Œå®ç°æ¨¡å‹çš„å‚ç›´é¢†åŸŸå®šåˆ¶åŒ–ã€‚

## æ”¯æŒçš„æ¨¡å‹
| Model            | Type | Download                                                                                                                                |
|------------------|------|-----------------------------------------------------------------------------------------------------------------------------------------|                                                                                                                                                                                         
| Llama-2-7b-hf    | Chat | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
| Llama-2-7b-chat-hf | Chat | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)                                                                                                                                                                                          |

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–é¡¹
```bash
git clone https://github.com/TAP-LLM/SplitFedLLM.git
cd SplitFedLLM
pip install -r requirements.txt
```
### æ•°æ®å‡†å¤‡

è¯·å°†è®­ç»ƒæ•°æ®é›†æŒ‰ç…§ç¤ºä¾‹æ–‡ä»¶æ ¼å¼è¿›è¡Œæ•´ç†ï¼ˆä¸€ä¸ªå•åˆ—çš„csvæ–‡ä»¶ï¼Œè¾“å…¥æ•´ä½“æ˜¯ä¸€å¥è¯ï¼Œå…¶ä¸­çš„ç‰¹æ®Štokençš„ä½¿ç”¨ä¹Ÿç¬¦åˆæ–‡ä»¶ç¤ºä¾‹ï¼š/data/train_test.csvï¼‰

### è®¾ç½®è„šæœ¬å‚æ•°

è¯·å°½å¯èƒ½ä¿è¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯çš„å‚æ•°è®¾ç½®ä¸€è‡´ï¼ˆåç»­å¯ä»¥è¿›è¡Œé€šä¿¡ä¼ é€’ï¼‰ï¼Œä»¥ä¸‹çš„å‚æ•°åŠ¡å¿…ä¿æŒä¸€è‡´ï¼š  
```bash
    --learning_rate  
    --num_train_epochs
    --warmup_steps 
    --load_in_bits
    --lora_r 
    --lora_alpha
    --target_modules
    --seed
    --block_size
    --deepspeed ds_config_zero2.json # deepspeedé…ç½®æ–‡ä»¶å¤ç”¨
    --bf16  
```


å®¢æˆ·ç«¯çš„--master_port='29501'å¦‚æœå•æœºæµ‹è¯•ä¸è¦æ”¹å›29500  


### é€šä¿¡


#### å¯åŠ¨æœåŠ¡
##### å¾®è°ƒï¼š
å®¢æˆ·ç«¯ï¼š`./finetune/sft-client`  
æœåŠ¡å™¨ï¼š`./finetune/sft-service`
å®¢æˆ·ç«¯ï¼ŒæœåŠ¡å™¨ç«¯éƒ½æ˜¯ä»¥å¯åŠ¨è„šæœ¬çš„æ–¹å¼å¯åŠ¨  
ä»¥å®¢æˆ·ç«¯ä¸ºä¾‹ï¼š
``` bash
    cd ./finetune/sft-client
    chmod +x ./finetune.sh
    sh ./finetune.sh
```
æ³¨æ„:
1. åœ¨å¯åŠ¨æœåŠ¡å‰ï¼Œè¯·å…ˆå°†æ‰€æœ‰çš„`'your_ip'`å­—æ®µä¿®æ”¹ä¸ºæ‚¨æœåŠ¡ç«¯ä¸æ¥æ”¶ç«¯çš„ipåœ°å€ã€‚
2. è¯·å…ˆå¯åŠ¨æœåŠ¡å™¨ç«¯ï¼Œæœ€å¥½ç­‰å¾…æœåŠ¡å™¨åé¦ˆ"ç­‰å¾…æ¥æ”¶"åå†å¯åŠ¨å®¢æˆ·ç«¯ã€‚
3. å¦‚æœä½¿ç”¨æ•°æ®é›†ä¸ºæœåŠ¡å™¨æ¥æ”¶ç«¯éƒ½å¯è§ï¼Œå¯ä»¥ä¿®æ”¹`finetune_clm_lora.py`å‚æ•°ä¸º`fast_finetune_clm_lora.py`ï¼Œæ³¨æ„æœåŠ¡å™¨ä¸æ¥æ”¶ç«¯åŒæ–¹éƒ½éœ€è¦ä¿®æ”¹ã€‚
4. å¦‚æœä½¿ç”¨æ•°æ®é›†ä¸ºæœåŠ¡å™¨ä¸å¯è§ï¼Œå³ä½¿ç”¨finetune_clm_lora.pyæ–‡ä»¶çš„è¯ï¼Œå°†/SplitFedLLM/finetune/sft-service/finetune.shæ–‡ä»¶ä¸­ çš„train_files ï¼Œvalidation_files å‚æ•°ä¿æŒé»˜è®¤å€¼ä¸å˜ï¼Œä»…ä¿®æ”¹clientç«¯çš„å‚æ•°





##### æ¨ç†ï¼š
å®¢æˆ·ç«¯ï¼š`./eval/client.py`  
æœåŠ¡å™¨ï¼š`./eval/service.py`  
å°†æœåŠ¡å™¨ç«¯å’Œå®¢æˆ·ç«¯çš„æ¨¡å‹å‚æ•°è·¯å¾„æ”¹å†™å¥½ï¼š  
```python
finetune_model_path = "your_finetune_model_path"  # loraå‚æ•°
base_model = "your_base_model_path"  # åŸæ¨¡å‹å‚æ•°
```
```bash
python service.py
python client.py  
```
æ³¨æ„:
1. åœ¨å¯åŠ¨æœåŠ¡å‰ï¼Œè¯·å…ˆå°†æ‰€æœ‰çš„`'your_ip'`å­—æ®µä¿®æ”¹ä¸ºæ‚¨æœåŠ¡ç«¯ä¸æ¥æ”¶ç«¯çš„ipåœ°å€ã€‚
2. è¯·å…ˆå¯åŠ¨æœåŠ¡å™¨ç«¯ï¼Œæœ€å¥½ç­‰å¾…æœåŠ¡å™¨åé¦ˆ"ç­‰å¾…æ¥æ”¶"åå†å¯åŠ¨å®¢æˆ·ç«¯ã€‚
#### ç»“æŸæœåŠ¡
ç»“æŸåï¼Œæ‰‹åŠ¨å…³é—­ç»ˆç«¯ã€‚

## TODO
åŠ å…¥æ–­ç‚¹ç»­è®­ï¼Œç›®å‰æš‚ä¸æ”¯æŒ

## å¼•ç”¨ 
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·å¼•ç”¨å¦‚ä¸‹è®ºæ–‡ã€‚
```
@misc{zheng2024safelylearningprivatedata,
      title={Safely Learning with Private Data: A Federated Learning Framework for Large Language Model}, 
      author={JiaYing Zheng and HaiNan Zhang and LingXiang Wang and WangJie Qiu and HongWei Zheng and ZhiMing Zheng},
      year={2024},
      eprint={2406.14898},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.14898}, 
}
```