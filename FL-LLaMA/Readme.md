# FL-LLaMA
![transformers](https://img.shields.io/badge/transformers->=4.38.0-greene)
![License](https://img.shields.io/badge/license-MIT-yellow)  
![Python](https://img.shields.io/badge/Python->=3.10.4-blue)  
 Read this in [English](README_en.md)

## æ”¯æŒçš„æ¨¡å‹
| Model            | Type | Download                                                                                                                                |
|------------------|------|-----------------------------------------------------------------------------------------------------------------------------------------|                                                                                                                                                                                         
| Llama-2-7b-hf    | Base | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
| Llama-2-7b-chat-hf | Chat | [ğŸ¤— Huggingface](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)                                                                                                                                                                                          |

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–é¡¹
```bash
git clone https://github.com/TAP-LLM/SplitFedLLM.git
cd SplitFedLLM/FL-LLaMa
pip install -r requirements.txt
```
### æ•°æ®å‡†å¤‡
1. SuperGLUE Benchmark
ä»»åŠ¡ç±»å‹ï¼šè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ï¼ŒåŒ…å« 8 ä¸ªå­ä»»åŠ¡ï¼Œè¦†ç›–å¸¸è¯†æ¨ç†ã€è¯­ä¹‰è§£æç­‰å¤æ‚è¯­è¨€æŒ‘æˆ˜ã€‚
æ•°æ®é›†è¯¦æƒ…ï¼š
ReCoRDï¼šé€šè¿‡æ¶ˆè§£æ®µè½ä¸­çš„æ­§ä¹‰å®ä½“è¯„ä¼°é˜…è¯»ç†è§£èƒ½åŠ›ã€‚
COPAï¼šé€šè¿‡é€‰æ‹©å‰æçš„æ›´å¯èƒ½åŸå› æˆ–ç»“æœè¯„ä¼°å› æœæ¨ç†èƒ½åŠ›ã€‚
WSCï¼šé€šè¿‡ä»£è¯æ¶ˆè§£è¯„ä¼°æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚
RTEï¼šé€šè¿‡åˆ¤æ–­å¥å­é—´çš„è•´å«ã€çŸ›ç›¾æˆ–æ— å…³å…³ç³»è¯„ä¼°æ–‡æœ¬è•´å«èƒ½åŠ›ã€‚
BoolQï¼šåŸºäºä¸Šä¸‹æ–‡å›ç­”æ˜¯ / å¦é—®é¢˜ã€‚
WiCï¼šé€šè¿‡åˆ¤æ–­å•è¯åœ¨ä¸¤ä¸ªå¥å­ä¸­çš„å«ä¹‰æ˜¯å¦ç›¸åŒè¯„ä¼°è¯ä¹‰æ¶ˆæ­§èƒ½åŠ›ã€‚
CBï¼šå¤„ç†å¤æ‚å¥å­ä¸­çš„è•´å«å…³ç³»ã€‚
MultiRCï¼šåŸºäºå¤šå¥å­ä¸Šä¸‹æ–‡å›ç­”å¤šç­”æ¡ˆé—®é¢˜ã€‚
å®˜æ–¹é“¾æ¥ï¼šhttps://super.gluebenchmark.com/tasks
ä¸‹è½½æ–¹å¼
wget https://dl.fbaipublicfiles.com/glue/superglue/data/v2/combined.zip

2. CoQA æ•°æ®é›†
ä»»åŠ¡ç±»å‹ï¼šå¯¹è¯å¼é—®ç­”ï¼ˆQAï¼‰ï¼Œèšç„¦å¤šè½®å¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§è¯„ä¼°ã€‚
æ•°æ®é›†è¯¦æƒ…ï¼š
åŒ…å« 8,000 å¤šä¸ªå¯¹è¯å’Œ 127,000 å¤šä¸ªé—®é¢˜ï¼Œè¦†ç›– 7 ä¸ªé¢†åŸŸã€‚
è¿‘åŠæ•°é—®é¢˜éœ€è¦æŒ‡ä»£æ¶ˆè§£å’Œè¯­ç”¨æ¨ç†ã€‚
å®˜æ–¹é“¾æ¥ï¼šhttps://stanfordnlp.github.io/coqa/
ä¸‹è½½æ–¹å¼ï¼š
wget https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json

3. XSum æ•°æ®é›†
ä»»åŠ¡ç±»å‹ï¼šæ‘˜è¦ç”Ÿæˆï¼Œè¯„ä¼°æ¨¡å‹å¯¹æ–°é—»æ–‡ç« çš„æç«¯å‹ç¼©èƒ½åŠ›ã€‚
æ•°æ®é›†è¯¦æƒ…ï¼š
åŒ…å« BBC æ–°é—»æ–‡ç« å’Œäººå·¥æ’°å†™çš„å•å¥æ‘˜è¦,è¦æ±‚æ¨¡å‹ç”Ÿæˆé«˜åº¦ç®€æ´çš„å•å¥æ‘˜è¦ã€‚
å®˜æ–¹é“¾æ¥ï¼šhttps://github.com/EdinburghNLP/XSum/tree/master
ä¸‹è½½æ–¹å¼ï¼š
#### é€šè¿‡ Hugging Face ä¸‹è½½
wget https://huggingface.co/datasets/EdinburghNLP/xsum/resolve/main/data/XSUM-EMNLP18-Summary-Data-Original.tar.gz?download=true
#### Xsumæ•°æ®æ‹†åˆ†
è¯¦è§ FedGLM-LLaMA/data/Xsum

### æ¨¡å‹å‡†å¤‡
æ³¨æ„ï¼šè¯·cdåˆ°å·¥ä½œç›®å½•  cd /FL-LLaMA

1.ä¸‹è½½Llama 2æ¨¡å‹ï¼Œä¿å­˜è‡³ FL-LLaMa/Centralized_Models/model_path/
2.åˆ†å‰²æ¨¡å‹ python src/SplitModel.py --SplitA --SplitB --SplitC
3.åˆ†å‰²Loraå‚æ•°, python src/SplitModel.py --LoraA --LoraB --LoraC --lora_r 16 --lora_alpha 32 ,ä¿è¯æ¯ä¸€æ¬¡ä»ç›¸åŒçš„loraæƒé‡è®­ç»ƒï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰

### å¯åŠ¨æœåŠ¡
è®¾ç½®è„šæœ¬å‚æ•°

å°†bashè„šæœ¬ä¸­çš„
    cd /home/zhangzishuai/SplitFederated-LLaMA/FL-LLaMA æ”¹æˆFL-LLaMAæ‰€åœ¨çš„è·¯å¾„
    ServerIP æ”¹æˆä½ çš„æœºå™¨çš„IPåœ°å€


è¯·ä¿è¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯çš„å‚æ•°è®¾ç½®ä¸€è‡´ï¼Œä»¥ä¸‹çš„å‚æ•°åŠ¡å¿…ä¿æŒä¸€è‡´ï¼š

```bash
    --batch_size
    --learning_rate
    --max_output_length  
    --lora_r 
    --lora_alpha
    --target_modules
    --seed
    --max_source_length
    --max_target_length
    --modelA_layers
    --modelC_layers
    --server_ip
```

#### å•å®¢æˆ·ç«¯å¾®è°ƒï¼š
`./scripts/Auto_train.sh`  

ä»¥å®¢æˆ·ç«¯ä¸ºä¾‹ï¼š
``` bash
    cd ./FL-LLaMA
    chmod +x ./scripts/Auto_train.sh
    bash ./scripts/Auto_train.sh
```
æ³¨æ„:
1. åœ¨å¯åŠ¨æœåŠ¡å‰ï¼Œè¯·å…ˆå°†æ‰€æœ‰çš„`'your_ip'`å­—æ®µä¿®æ”¹ä¸ºæ‚¨æœåŠ¡ç«¯ä¸æ¥æ”¶ç«¯çš„ipåœ°å€ã€‚
2. å¦‚æœä¸é€šè¿‡Auto_train.sh,æ‰‹åŠ¨å¯åŠ¨æœåŠ¡å™¨ç«¯å’Œå®¢æˆ·ç«¯ï¼Œæœ€å¥½ç­‰å¾…æœåŠ¡å™¨åé¦ˆ"Starting Flower server, config: ServerConfig(num_rounds=None, round_timeout=None)"åå†å¯åŠ¨å®¢æˆ·ç«¯ã€‚
3. train_fileåªåœ¨flclient_id_0.pyä¸ºTrainæ¨¡å¼ä¸‹æœ‰å€¼ï¼Œå¦åˆ™ä¸ºNone ;test_fileåªåœ¨flclient_id_0.pyä¸ºTestæ¨¡å¼ä¸‹æœ‰å€¼
4. å•å®¢æˆ·ç«¯å¾®è°ƒæ—¶ï¼Œflserverçš„client_countå‚æ•°å¿…é¡»ä¸º1ï¼Œå¦åˆ™ä¼šä¸€ç›´å¡ä½

#### å¤šå®¢æˆ·ç«¯å¾®è°ƒï¼š
`./scripts/Auto_BatchTrain.sh`  

ä»¥å®¢æˆ·ç«¯ä¸ºä¾‹ï¼š
``` bash
    cd ./FL-LLaMA
    chmod +x ./scripts/Auto_Batchtrain.sh
    bash ./scripts/Auto_Batchtrain.sh
```
æ³¨æ„:
1. åœ¨å¯åŠ¨æœåŠ¡å‰ï¼Œè¯·å…ˆå°†æ‰€æœ‰çš„`'your_ip'`å­—æ®µä¿®æ”¹ä¸ºæ‚¨æœåŠ¡ç«¯ä¸æ¥æ”¶ç«¯çš„ipåœ°å€ã€‚
2. å¦‚æœä¸é€šè¿‡train.sh,æ‰‹åŠ¨å¯åŠ¨æœåŠ¡å™¨ç«¯å’Œå®¢æˆ·ç«¯ï¼Œæœ€å¥½ç­‰å¾…æœåŠ¡å™¨åé¦ˆ"Starting Flower server, config: ServerConfig(num_rounds=None, round_timeout=None)"åå†å¯åŠ¨å®¢æˆ·ç«¯ã€‚


#### æ¨ç†ï¼š

```bash
    cd ./FL-LLaMA
    chmod +x ./scripts/Inference.sh
    bash ./scripts/Inference.sh
```
æ³¨æ„:
1. åœ¨å¯åŠ¨æœåŠ¡å‰ï¼Œè¯·å…ˆå°†æ‰€æœ‰çš„`'your_ip'`å­—æ®µä¿®æ”¹ä¸ºæ‚¨æœåŠ¡ç«¯ä¸æ¥æ”¶ç«¯çš„ipåœ°å€ã€‚
2. å¦‚æœä¸é€šè¿‡train.sh,æ‰‹åŠ¨å¯åŠ¨æœåŠ¡å™¨ç«¯å’Œå®¢æˆ·ç«¯ï¼Œæœ€å¥½ç­‰å¾…æœåŠ¡å™¨åé¦ˆ"Starting Flower server, config: ServerConfig(num_rounds=None, round_timeout=None)"åå†å¯åŠ¨å®¢æˆ·ç«¯ã€‚
#### ç»“æŸæœåŠ¡
ç»“æŸåï¼Œæ‰‹åŠ¨å…³é—­ç»ˆç«¯ã€‚

æ›´å¤šåŠŸèƒ½è¯·è‡ªç”±æ¢ç´¢

## TODO
åŠ å…¥æ–­ç‚¹ç»­è®­ï¼Œç›®å‰æš‚ä¸æ”¯æŒ

## å¼•ç”¨ 
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·å¼•ç”¨å¦‚ä¸‹è®ºæ–‡ã€‚
```
@article{zhang2025federated,
  title={A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability},
  author={Zhang, Zishuai and Zhang, Hainan and Zheng, Jiaying and Wang, Ziwei and Tong, Yongxin and Dong, Jin and Zheng, Zhiming},
  journal={arXiv preprint arXiv:2505.15683},
  year={2025}
}

@misc{zheng2024safelylearningprivatedata,
      title={Safely Learning with Private Data: A Federated Learning Framework for Large Language Model}, 
      author={JiaYing Zheng and HaiNan Zhang and LingXiang Wang and WangJie Qiu and HongWei Zheng and ZhiMing Zheng},
      year={2024},
      eprint={2406.14898},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.14898}, 
}
```